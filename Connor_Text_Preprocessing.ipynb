{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8M0atCN6kVmt3X/rlat80",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trueconnor/BUS-Z-798/blob/main/Connor_Text_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AW7YUrx2KFEN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0bzUkFMpase"
      },
      "source": [
        "# Get external files\n",
        "!mkdir -p texts\n",
        "!wget -q https://www.dropbox.com/s/5ibk0k4mibcq3q6/AussieTop100private.zip?dl=1 -O ./texts/AussieTop100private.zip\n",
        "!unzip -qq -n -d ./texts/ ./texts/AussieTop100private.zip\n",
        "\n",
        "# Standard library imports\n",
        "import glob, string\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "# 3rd party imports\n",
        "import nltk, nltk.sentiment\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.sentiment.util import mark_negation\n",
        "import pandas as pd\n",
        "\n",
        "# Downloads nltk corpora for preprocessing tasks\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "\n",
        "# Creates path variables to texts\n",
        "about_dir = Path.cwd() / \"texts\" / \"About\"\n",
        "pr_dir = Path.cwd() / \"texts\" / \"PR\"\n",
        "dirs_to_load = [about_dir, pr_dir]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tRfj6VH2LsEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dY34zooi0vM"
      },
      "source": [
        "# Load texts\n",
        "\n",
        "# Load texts\n",
        "texts = [\n",
        "    {'text_type': 'news', 'text_id': '1', 'text': 'This is a news article about a company.'},\n",
        "    {'text_type': 'news', 'text_id': '2', 'text': 'This is another news article about a company.'},\n",
        "    {'text_type': 'news', 'text_id': '3', 'text': 'This is a third news article about a company.'}\n",
        "]\n",
        "\n",
        "\n",
        "for directory in dirs_to_load:\n",
        "  for file in glob.glob(f\"{directory}/*.txt\"):\n",
        "    with open(file, 'r') as infile:\n",
        "      text_type = file.split(\"/\")[-2]\n",
        "      text_id = file.split(\"/\")[-1]\n",
        "      texts.append({'text_type': text_type, 'text_id': text_id, 'text': infile.read()})\n",
        "\n",
        "# Text Preprocessing Pipeline\n",
        "for id, article in enumerate(texts):\n",
        "  if id == 0:\n",
        "    print(\"---Original--\")\n",
        "    print(article['text'][0:367])\n",
        "\n",
        "  # Segmentation\n",
        "  article['text'] = nltk.tokenize.sent_tokenize(article['text'])\n",
        "  if id == 0:\n",
        "    print(\"\\n---After segmentation--\")\n",
        "    print(article['text'][0:2])\n",
        "\n",
        "  # Tokenization\n",
        "  article['text'] = [nltk.tokenize.word_tokenize(sentence) for sentence in article['text']]\n",
        "  if id == 0:\n",
        "    print(\"\\n---After tokenization--\")\n",
        "    print(article['text'][0:2])\n",
        "\n",
        "  # Case conversion\n",
        "  for sent_id, sentence in enumerate(article['text']):\n",
        "    article['text'][sent_id] = [word.lower() for word in sentence]\n",
        "  if id == 0:\n",
        "    print(\"\\n---After case conversion--\")\n",
        "    print(article['text'][0:2])\n",
        "\n",
        "  # Non-word character removal\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  for sent_id, sentence in enumerate(article['text']):\n",
        "    article['text'][sent_id] = [word.translate(table) for word in sentence if word.translate(table) and not word.isdigit()]\n",
        "  if id == 0:\n",
        "    print(\"\\n---After non-word character removal--\")\n",
        "    print(article['text'][0:2])\n",
        "\n",
        "  # Token replacement\n",
        "  translation_dict = {\"'s\": \"is\",\n",
        "                      \"n't\": \"not\",\n",
        "                      \"IT\": \"Information Technology\"}\n",
        "  for sent_id, sentence in enumerate(article['text']):\n",
        "    article['text'][sent_id] = [word if word not in translation_dict else translation_dict[word] for word in sentence]\n",
        "    article['text'][sent_id] = nltk.sentiment.util.mark_negation(sentence)\n",
        "  if id == 0:\n",
        "    print(\"\\n---After token replacement removal--\")\n",
        "    print(article['text'][0:2])\n",
        "\n",
        "  # Stop word removal\n",
        "  stop_words = nltk.corpus.stopwords.words(\"english\")\n",
        "  for sent_id, sentence in enumerate(article['text']):\n",
        "    article['text'][sent_id] = [word for word in sentence if word not in stop_words]\n",
        "  if id == 0:\n",
        "    print(\"\\n---After stop word removal--\")\n",
        "    print(article['text'][0:2])\n",
        "\n",
        "  # Lemmatization\n",
        "  pos_map = {'J': wn.ADJ, 'N': wn.NOUN, 'R': wn.ADV, 'V': wn.VERB}\n",
        "  lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "  for sent_id, sentence in enumerate(article['text']):\n",
        "    article['text'][sent_id] = [lemmatizer.lemmatize(a, pos_map.get(b[0], wn.NOUN)) for a, b in nltk.pos_tag(sentence)]\n",
        "  if id == 0:\n",
        "    print(\"\\n---After lemmatization--\")\n",
        "    print(article['text'][0:2])\n",
        "\n",
        "corpus_df = pd.DataFrame(texts)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}